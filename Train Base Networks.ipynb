{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i5pcljKozO0R",
    "ExecuteTime": {
     "end_time": "2024-08-24T12:50:55.754505400Z",
     "start_time": "2024-08-24T12:50:48.013711200Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-24T12:50:55.875516400Z",
     "start_time": "2024-08-24T12:50:55.755508100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qxG7fi2_dqcJ",
    "ExecuteTime": {
     "end_time": "2024-08-24T13:21:35.678273400Z",
     "start_time": "2024-08-24T13:21:35.660800600Z"
    }
   },
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def int_to_onehot(self, indx):\n",
    "        one_hot = torch.zeros(range_y).float()\n",
    "        one_hot[int(indx) - int(min_y)] = 1.0  # Adjusting class labels to be zero-based\n",
    "        return one_hot\n",
    "\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.int_to_onehot(self.y[idx])\n",
    "\n",
    "def train_model(net, X_train, Y_train, X_val, Y_val):\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(net.parameters(), lr=1e-3)\n",
    "\n",
    "    ds = Dataset(X_train, Y_train)\n",
    "    train_loader = torch.utils.data.DataLoader(ds, batch_size=64, shuffle=True)\n",
    "    net = net.float()\n",
    "\n",
    "    for epoch in range(100):\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for i, batch in enumerate(train_loader):\n",
    "            curr_x, curr_y = batch\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(curr_x.to(device).float())  # Forward pass\n",
    "            curr_y = torch.max(curr_y, 1)[1]  # Target labels conversion to 1D\n",
    "            loss = loss_func(outputs, curr_y.to(device))\n",
    "            # Backpropagation and network parameters' update.\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            _, pred = torch.max(outputs, -1)\n",
    "\n",
    "        train_output = torch.argmax(netX(torch.Tensor(X_train).to(device).float()).detach().cpu(),1)\n",
    "        train_acc = accuracy_score(train_output, Y_train)\n",
    "        validation_output = torch.argmax(netX(torch.Tensor(X_val).to(device).float()).detach().cpu(),1)\n",
    "        validation_acc = accuracy_score(validation_output, Y_val)\n",
    "\n",
    "        # if (epoch == 99):\n",
    "        #     print(\"Epoch : {}, train acc : {}, train loss : {}, validation acc : {}\".format(epoch + 1, train_acc, running_loss, validation_acc))\n",
    "        # running_loss = 0.00\n",
    "    \n",
    "    return optimizer, loss_func\n",
    "\n",
    "\n",
    "class NetX(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(NetX, self).__init__()\n",
    "        layers = []\n",
    "        layers_count = np.random.randint(1, 7)\n",
    "        layer_neuron_size = list(range(100, 1100, 100))\n",
    "        previous_layer_size = X.shape[1]\n",
    "        \n",
    "        for i in range(layers_count):\n",
    "            layer_size = np.random.choice(layer_neuron_size, 1)[0]\n",
    "            \n",
    "            norm = np.random.random()\n",
    "            \n",
    "            if norm <= 0.33:\n",
    "                layers.append(nn.Dropout())\n",
    "            elif norm <= 0.66:\n",
    "                layers.append(nn.BatchNorm1d(previous_layer_size))\n",
    "                        \n",
    "            layers.append(nn.Linear(previous_layer_size, layer_size))\n",
    "            \n",
    "            activation = np.random.random()\n",
    "\n",
    "                  \n",
    "            if activation < 0.05:\n",
    "                layers.append(nn.Tanh())\n",
    "            elif activation < 0.1:\n",
    "                layers.append(nn.Sigmoid())\n",
    "            else: \n",
    "                # TODO: Original ReLU usage causes NaNs in net_ac, layer_ac. Compare Leaky ReLU to ELU!\n",
    "                # ELU works fine, evaluating SiLU [aka Swish = x * sigmoid(x)]\n",
    "                # layers.append(nn.ReLU())\n",
    "                #layers.append(nn.ELU())\n",
    "                layers.append(nn.SiLU())\n",
    "                \n",
    "                \n",
    "                \n",
    "            previous_layer_size = layer_size\n",
    "                \n",
    "        layers.append(nn.Linear(previous_layer_size, int(range_y)))\n",
    "        layers.append(nn.Softmax(dim=1))\n",
    "\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-F0JKvLO6Uco",
    "ExecuteTime": {
     "end_time": "2024-08-24T18:22:08.433319500Z",
     "start_time": "2024-08-24T18:22:08.417970100Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ordered by size, increasing order \n",
    "dataset_names=[\n",
    "    # 'rmftsa_sleepdata',\n",
    "    # 'diggle_table_a2',\n",
    "    # 'no2',\n",
    "    # 'pm10',\n",
    "    # 'disclosure_z',\n",
    "    # 'diabetes',\n",
    "    # 'mfeat-morphological',\n",
    "    # 'cardiotocography',\n",
    "    # 'nursery',\n",
    "    # 'phoneme',\n",
    "    ## 'bank-marketing',\n",
    "    #'kr-vs-k',\n",
    "    ##'kropt',\n",
    "    ##'jungle_chess_2pcs_raw_endgame_complete',\n",
    "    '2dplanes',\n",
    "    'tamilnadu-electricityarff',\n",
    "    'house_8L',\n",
    "    'mfeat-karhunen',\n",
    "    'amazon_employee_access',\n",
    "    'satimage',\n",
    "    'ailerons',\n",
    "    'fried',\n",
    "    'first-order-theorem-proving',\n",
    "    'electricity-normalized',\n",
    "    'CreditCardSubset',    \n",
    "    'airlines',    \n",
    "    'nomao',               \n",
    "    # 'mnist',\n",
    "    # 'cifar-100',\n",
    "    # 'cifar-10',        \n",
    "    # 'svhn',    \n",
    "]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "FaNSSoo_YCMB",
    "outputId": "977d8c69-ff21-4ce4-e48e-bc7f3c774c43",
    "ExecuteTime": {
     "end_time": "2024-08-25T00:32:49.348199800Z",
     "start_time": "2024-08-24T18:22:11.776508200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================================================\n",
      "Starting dataset  kropt\n",
      "dataset kropt was skipped due to non-numeric column\n",
      "==================================================================================\n",
      "Starting dataset  jungle_chess_2pcs_raw_endgame_complete\n",
      "dataset jungle_chess_2pcs_raw_endgame_complete was skipped due to non-numeric column\n",
      "==================================================================================\n",
      "Starting dataset  2dplanes\n",
      "Starting net  1\n",
      "Saved net  1\n",
      "Starting net  2\n",
      "Saved net  2\n",
      "Starting net  3\n",
      "Saved net  3\n",
      "Starting net  4\n",
      "Saved net  4\n",
      "Starting net  5\n",
      "Saved net  5\n",
      "==================================================================================\n",
      "Starting dataset  tamilnadu-electricityarff\n",
      "Starting net  1\n",
      "Saved net  1\n",
      "Starting net  2\n",
      "Saved net  2\n",
      "Starting net  3\n",
      "Saved net  3\n",
      "Starting net  4\n",
      "Saved net  4\n",
      "Starting net  5\n",
      "Saved net  5\n",
      "==================================================================================\n",
      "Starting dataset  house_8L\n",
      "Starting net  1\n",
      "Saved net  1\n",
      "Starting net  2\n",
      "Saved net  2\n",
      "Starting net  3\n",
      "Saved net  3\n",
      "Starting net  4\n",
      "Saved net  4\n",
      "Starting net  5\n",
      "Saved net  5\n",
      "==================================================================================\n",
      "Starting dataset  mfeat-karhunen\n",
      "Starting net  1\n",
      "Saved net  1\n",
      "Starting net  2\n",
      "Saved net  2\n",
      "Starting net  3\n",
      "Saved net  3\n",
      "Starting net  4\n",
      "Saved net  4\n",
      "Starting net  5\n",
      "Saved net  5\n",
      "==================================================================================\n",
      "Starting dataset  amazon_employee_access\n",
      "Starting net  1\n",
      "Saved net  1\n",
      "Starting net  2\n",
      "Saved net  2\n",
      "Starting net  3\n",
      "Saved net  3\n",
      "Starting net  4\n",
      "Saved net  4\n",
      "Starting net  5\n",
      "Saved net  5\n",
      "==================================================================================\n",
      "Starting dataset  satimage\n",
      "Starting net  1\n",
      "Saved net  1\n",
      "Starting net  2\n",
      "Saved net  2\n",
      "Starting net  3\n",
      "Saved net  3\n",
      "Starting net  4\n",
      "Saved net  4\n",
      "Starting net  5\n",
      "Saved net  5\n",
      "==================================================================================\n",
      "Starting dataset  ailerons\n",
      "Starting net  1\n",
      "Saved net  1\n",
      "Starting net  2\n",
      "Saved net  2\n",
      "Starting net  3\n",
      "Saved net  3\n",
      "Starting net  4\n",
      "Saved net  4\n",
      "Starting net  5\n",
      "Saved net  5\n",
      "==================================================================================\n",
      "Starting dataset  fried\n",
      "Starting net  1\n",
      "Saved net  1\n",
      "Starting net  2\n",
      "Saved net  2\n",
      "Starting net  3\n",
      "Saved net  3\n",
      "Starting net  4\n",
      "Saved net  4\n",
      "Starting net  5\n",
      "Saved net  5\n",
      "==================================================================================\n",
      "Starting dataset  first-order-theorem-proving\n",
      "Starting net  1\n",
      "Saved net  1\n",
      "Starting net  2\n",
      "Saved net  2\n",
      "Starting net  3\n",
      "Saved net  3\n",
      "Starting net  4\n",
      "Saved net  4\n",
      "Starting net  5\n",
      "Saved net  5\n",
      "==================================================================================\n",
      "Starting dataset  electricity-normalized\n",
      "dataset electricity-normalized was skipped due to non-numeric column\n",
      "==================================================================================\n",
      "Starting dataset  CreditCardSubset\n",
      "Starting net  1\n",
      "Saved net  1\n",
      "Starting net  2\n",
      "Saved net  2\n",
      "Starting net  3\n",
      "Saved net  3\n",
      "Starting net  4\n",
      "Saved net  4\n",
      "Starting net  5\n",
      "Saved net  5\n",
      "==================================================================================\n",
      "Starting dataset  airlines\n",
      "dataset airlines was skipped due to non-numeric column\n",
      "==================================================================================\n",
      "Starting dataset  nomao\n",
      "Starting net  1\n",
      "Saved net  1\n",
      "Starting net  2\n",
      "Saved net  2\n",
      "Starting net  3\n",
      "Saved net  3\n",
      "Starting net  4\n",
      "Saved net  4\n",
      "Starting net  5\n",
      "Saved net  5\n",
      "==================================================================================\n",
      "Starting dataset  mnist\n",
      "==================================================================================\n",
      "Starting dataset  cifar-100\n",
      "==================================================================================\n",
      "Starting dataset  cifar-10\n",
      "==================================================================================\n",
      "Starting dataset  svhn\n"
     ]
    }
   ],
   "source": [
    "for dataset_name in dataset_names:\n",
    "  print(\"==================================================================================\")\n",
    "  print(\"Starting dataset \", dataset_name)\n",
    "  if not dataset_name in ['cifar-10', 'cifar-100', 'mnist', 'svhn']:\n",
    "    PATH = 'C:/Users/idopa/Documents/BGU/MSc/SPECTRA-CompressionAgent/datasets/NEON-csv/'\n",
    "      \n",
    "  else:\n",
    "    continue\n",
    "    #PATH = 'C:/Users/idopa/Documents/BGU/MSc/SPECTRA-CompressionAgent/datasets/SPECTRA-csv/'\n",
    "  \n",
    "  df = pd.read_csv(PATH+dataset_name+'.csv')\n",
    "  \n",
    "  try:\n",
    "    for col in df.columns:\n",
    "        if col in ['binaryClass', 'unemployed']:\n",
    "            df[col] = df[col].replace({'P': 1, 'N': 0})\n",
    "        df[col] = df[col].astype(float)\n",
    "  except ValueError:  # non-numeric column, skip for now\n",
    "    print(f\"dataset {dataset_name} was skipped due to non-numeric column\")\n",
    "    continue\n",
    "      \n",
    "  PATH += f'{dataset_name}/'\n",
    "  os.mkdir(PATH)    \n",
    "\n",
    "  dataset = df.values\n",
    "  X = dataset[:,0:dataset.shape[1] - 1]\n",
    "  Y = dataset[:,dataset.shape[1] - 1]\n",
    "\n",
    "  min_y = min(Y)\n",
    "  max_y = max(Y)\n",
    "\n",
    "  range_y = int(max_y - min_y + 1)\n",
    "\n",
    "  min_max_scaler = preprocessing.MinMaxScaler()\n",
    "  X_scale = min_max_scaler.fit_transform(X)\n",
    " \n",
    "  X_train, X_val, Y_train, Y_val = train_test_split(X_scale, Y, test_size=0.3, random_state=0)\n",
    "  \n",
    "  pd.DataFrame(X_train).to_csv(PATH + \"./X_train.csv\", index = False)\n",
    "  pd.DataFrame(X_val).to_csv(PATH + \"./X_val.csv\", index = False)\n",
    "  pd.DataFrame(Y_train).to_csv(PATH + \"./Y_train.csv\", index = False)\n",
    "  pd.DataFrame(Y_val).to_csv(PATH + \"./Y_val.csv\", index = False)\n",
    "\n",
    "  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "  for i in range(5):\n",
    "      print(\"Starting net \", i+1)\n",
    "      netX = NetX()\n",
    "      netX.to(device)\n",
    "      opt, l = train_model(netX, X_train, Y_train, X_val, Y_val)\n",
    "\n",
    "      checkpoint = {'model': netX,\n",
    "                    'state_dict': netX.state_dict(),\n",
    "                    'optimizer' : opt.state_dict(),\n",
    "                    'mission_type': \"Classification\",\n",
    "                    'loss': l  \n",
    "                  }\n",
    "                    \n",
    "      torch.save(checkpoint, PATH + 'netX{}model.pt'.format(i+1))\n",
    "      print(\"Saved net \", i+1)\n",
    "      # print(\"===================================================================\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "colab_type": "code",
    "id": "Egeay-H9trL3",
    "outputId": "cb10e681-091f-48cd-ca29-1e9bc3224b57",
    "ExecuteTime": {
     "end_time": "2024-06-16T22:26:03.212936600Z",
     "start_time": "2024-06-16T22:26:03.195430200Z"
    }
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import zipfile\n",
    "\n",
    "# def zipdir(path, ziph):\n",
    "#     # ziph is zipfile handle\n",
    "#     for root, dirs, files in os.walk(path):\n",
    "#         for file in files:\n",
    "#             ziph.write(os.path.join(root, file))\n",
    "\n",
    "# zipf = zipfile.ZipFile('/content/drive/My Drive/Fully Connected Training/Classification/Classification.zip', 'w', zipfile.ZIP_DEFLATED)\n",
    "# zipdir('/content/drive/My Drive/Fully Connected Training/Classification/', zipf)\n",
    "# zipf.close()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Classification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "name": "conda_cnn",
   "language": "python",
   "display_name": "Python (conda_cnn)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
