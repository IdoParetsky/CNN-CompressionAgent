{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i5pcljKozO0R",
    "ExecuteTime": {
     "end_time": "2024-06-16T22:25:19.449518400Z",
     "start_time": "2024-06-16T22:25:19.436011700Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-16T22:25:19.468466Z",
     "start_time": "2024-06-16T22:25:19.446521300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qxG7fi2_dqcJ",
    "ExecuteTime": {
     "end_time": "2024-06-16T22:25:19.478229700Z",
     "start_time": "2024-06-16T22:25:19.470467700Z"
    }
   },
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def int_to_onehot(self, indx):\n",
    "        one_hot = torch.zeros(range_y).float()\n",
    "        one_hot[int(indx) - int(min_y)] = 1.0  # Adjusting class labels to be zero-based\n",
    "        return one_hot\n",
    "\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.int_to_onehot(self.y[idx])\n",
    "\n",
    "def train_model(net, X_train, Y_train, X_val, Y_val):\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(net.parameters(), lr=1e-3)\n",
    "\n",
    "    ds = Dataset(X_train, Y_train)\n",
    "    train_loader = torch.utils.data.DataLoader(ds, batch_size=64, shuffle=True)\n",
    "    net = net.float()\n",
    "\n",
    "    for epoch in range(100):\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for i, batch in enumerate(train_loader):\n",
    "            curr_x, curr_y = batch\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(curr_x.to(device).float())  # Forward pass\n",
    "            curr_y = torch.max(curr_y, 1)[1]  # Target labels conversion to 1D\n",
    "            loss = loss_func(outputs, curr_y.to(device))\n",
    "            # Backpropagation and network parameters' update.\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            _, pred = torch.max(outputs, -1)\n",
    "\n",
    "        train_output = torch.argmax(netX(torch.Tensor(X_train).to(device).float()).detach().cpu(),1)\n",
    "        train_acc = accuracy_score(train_output, Y_train)\n",
    "        validation_output = torch.argmax(netX(torch.Tensor(X_val).to(device).float()).detach().cpu(),1)\n",
    "        validation_acc = accuracy_score(validation_output, Y_val)\n",
    "\n",
    "        # if (epoch == 99):\n",
    "        #     print(\"Epoch : {}, train acc : {}, train loss : {}, validation acc : {}\".format(epoch + 1, train_acc, running_loss, validation_acc))\n",
    "        # running_loss = 0.00\n",
    "    \n",
    "    return optimizer, loss_func\n",
    "\n",
    "\n",
    "class NetX(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(NetX, self).__init__()\n",
    "        layers = []\n",
    "        layers_count = np.random.randint(1, 7)\n",
    "        layer_neuron_size = list(range(100, 1100, 100))\n",
    "        previous_layer_size = X.shape[1]\n",
    "        \n",
    "        for i in range(layers_count):\n",
    "            layer_size = np.random.choice(layer_neuron_size, 1)[0]\n",
    "            \n",
    "            norm = np.random.random()\n",
    "            \n",
    "            if norm <= 0.33:\n",
    "                layers.append(nn.Dropout())\n",
    "            elif norm <= 0.66:\n",
    "                layers.append(nn.BatchNorm1d(previous_layer_size))\n",
    "                        \n",
    "            layers.append(nn.Linear(previous_layer_size, layer_size))\n",
    "            \n",
    "            activation = np.random.random()\n",
    "\n",
    "                  \n",
    "            if activation < 0.05:\n",
    "                layers.append(nn.Tanh())\n",
    "            elif activation < 0.1:\n",
    "                layers.append(nn.Sigmoid())\n",
    "            else: \n",
    "                # TODO: Original ReLU usage causes NaNs in net_ac, layer_ac. Compare Leaky ReLU to ELU!\n",
    "                # layers.append(nn.ReLU())\n",
    "                layers.append(nn.ELU())\n",
    "                \n",
    "            previous_layer_size = layer_size\n",
    "                \n",
    "        layers.append(nn.Linear(previous_layer_size, int(range_y)))\n",
    "        layers.append(nn.Softmax(dim=1))\n",
    "\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-F0JKvLO6Uco",
    "ExecuteTime": {
     "end_time": "2024-06-16T22:25:19.500321800Z",
     "start_time": "2024-06-16T22:25:19.478836600Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset_names=[\n",
    "  # '‪analcatdata_lawsuit‬‏',\n",
    "  # 'ar4',\n",
    "  # 'baseball',\n",
    "  #'bodyfat',\n",
    "  # 'chatfield_4',\n",
    "  # 'chscase_vine1',\n",
    "  # 'diabetes',\n",
    "  # 'diggle_table_a2',\n",
    "  # 'disclosure_z',\n",
    "  # 'elusage',\n",
    "  # 'fri_c0_250_5',\n",
    "  # 'kc3',\n",
    "  # 'lowbwt',\n",
    "  # 'lupus',\n",
    "  # # Important - with many classes!!\n",
    "  #'mfeat-karhunen',\n",
    "  #'mfeat-morphological',\n",
    "  #'no2',\n",
    "  #'pm10',\n",
    "  #'prnn_synth',\n",
    "  #'rabe_131',\n",
    "  #'rmftsa_sleepdata',\n",
    "  #'schlvote',\n",
    "  #'tae',\n",
    "  'teachingAssistant',\n",
    "  #'transplant',\n",
    "  #'triazines',\n",
    "  #'veteran'\n",
    "]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "FaNSSoo_YCMB",
    "outputId": "977d8c69-ff21-4ce4-e48e-bc7f3c774c43",
    "ExecuteTime": {
     "end_time": "2024-06-16T22:26:03.191919500Z",
     "start_time": "2024-06-16T22:25:19.496799400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================================================\n",
      "Starting dataset  teachingAssistant\n",
      "Starting net  0\n",
      "Saved net  0\n",
      "Starting net  1\n",
      "Saved net  1\n",
      "Starting net  2\n",
      "Saved net  2\n",
      "Starting net  3\n",
      "Saved net  3\n",
      "Starting net  4\n",
      "Saved net  4\n",
      "Starting net  5\n",
      "Saved net  5\n",
      "Starting net  6\n",
      "Saved net  6\n",
      "Starting net  7\n",
      "Saved net  7\n",
      "Starting net  8\n",
      "Saved net  8\n",
      "Starting net  9\n",
      "Saved net  9\n",
      "Starting net  10\n",
      "Saved net  10\n",
      "Starting net  11\n",
      "Saved net  11\n",
      "Starting net  12\n",
      "Saved net  12\n",
      "Starting net  13\n",
      "Saved net  13\n",
      "Starting net  14\n",
      "Saved net  14\n",
      "Starting net  15\n",
      "Saved net  15\n",
      "Starting net  16\n",
      "Saved net  16\n",
      "Starting net  17\n",
      "Saved net  17\n",
      "Starting net  18\n",
      "Saved net  18\n",
      "Starting net  19\n",
      "Saved net  19\n",
      "Starting net  20\n",
      "Saved net  20\n",
      "Starting net  21\n",
      "Saved net  21\n",
      "Starting net  22\n",
      "Saved net  22\n",
      "Starting net  23\n",
      "Saved net  23\n",
      "Starting net  24\n",
      "Saved net  24\n",
      "Starting net  25\n",
      "Saved net  25\n",
      "Starting net  26\n",
      "Saved net  26\n",
      "Starting net  27\n",
      "Saved net  27\n",
      "Starting net  28\n",
      "Saved net  28\n",
      "Starting net  29\n",
      "Saved net  29\n"
     ]
    }
   ],
   "source": [
    "for dataset_name in dataset_names:\n",
    "  print(\"==================================================================================\")\n",
    "  print(\"Starting dataset \", dataset_name)\n",
    "  PATH = 'C:/Users/idopa/Documents/BGU/MSc/CNN-CompressionAgent/datasets/' + dataset_name + '/'\n",
    "\n",
    "  df = pd.read_csv(PATH+dataset_name+'.csv')\n",
    "\n",
    "  if dataset_name == 'bodyfat':\n",
    "    df['binaryClass'] = df['binaryClass'].replace({'P': 1, 'N': 0})\n",
    "\n",
    "  for col in df.columns:\n",
    "    df[col] = df[col].astype(float)\n",
    "\n",
    "\n",
    "  dataset = df.values\n",
    "  X = dataset[:,0:dataset.shape[1] - 1]\n",
    "  Y = dataset[:,dataset.shape[1] - 1]\n",
    "\n",
    "  min_y = min(Y)\n",
    "  max_y = max(Y)\n",
    "\n",
    "  range_y = int(max_y - min_y + 1)\n",
    "\n",
    "  min_max_scaler = preprocessing.MinMaxScaler()\n",
    "  X_scale = min_max_scaler.fit_transform(X)\n",
    " \n",
    "  X_train, X_val, Y_train, Y_val = train_test_split(X_scale, Y, test_size=0.3, random_state=0)\n",
    "  \n",
    "  pd.DataFrame(X_train).to_csv(PATH + \"./X_train.csv\", index = False)\n",
    "  pd.DataFrame(X_val).to_csv(PATH + \"./X_val.csv\", index = False)\n",
    "  pd.DataFrame(Y_train).to_csv(PATH + \"./Y_train.csv\", index = False)\n",
    "  pd.DataFrame(Y_val).to_csv(PATH + \"./Y_val.csv\", index = False)\n",
    "\n",
    "  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "  for i in range(30):\n",
    "      print(\"Starting net \", i)\n",
    "      netX = NetX()\n",
    "      netX.to(device)\n",
    "      opt, l = train_model(netX, X_train, Y_train, X_val, Y_val)\n",
    "\n",
    "      checkpoint = {'model': netX,\n",
    "                    'state_dict': netX.state_dict(),\n",
    "                    'optimizer' : opt.state_dict(),\n",
    "                    'mission_type': \"Classification\",\n",
    "                    'loss': l  \n",
    "                  }\n",
    "                    \n",
    "      torch.save(checkpoint, PATH + 'netX{}model.pt'.format(i))\n",
    "      print(\"Saved net \", i)\n",
    "      # print(\"===================================================================\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "colab_type": "code",
    "id": "Egeay-H9trL3",
    "outputId": "cb10e681-091f-48cd-ca29-1e9bc3224b57",
    "ExecuteTime": {
     "end_time": "2024-06-16T22:26:03.212936600Z",
     "start_time": "2024-06-16T22:26:03.195430200Z"
    }
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import zipfile\n",
    "\n",
    "# def zipdir(path, ziph):\n",
    "#     # ziph is zipfile handle\n",
    "#     for root, dirs, files in os.walk(path):\n",
    "#         for file in files:\n",
    "#             ziph.write(os.path.join(root, file))\n",
    "\n",
    "# zipf = zipfile.ZipFile('/content/drive/My Drive/Fully Connected Training/Classification/Classification.zip', 'w', zipfile.ZIP_DEFLATED)\n",
    "# zipdir('/content/drive/My Drive/Fully Connected Training/Classification/', zipf)\n",
    "# zipf.close()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Classification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "name": "conda_cnn",
   "language": "python",
   "display_name": "Python (conda_cnn)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
